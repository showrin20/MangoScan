{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 12161182,
          "sourceType": "datasetVersion",
          "datasetId": 7659126
        }
      ],
      "dockerImageVersionId": 31041,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "MangoScan: Real-Time Mango Species Detection Syste",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/showrin20/MangoScan/blob/main/MangoScan_Real_Time_Mango_Species_Detection_Syste.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from glob import glob\n",
        "import random\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-14T06:47:35.18521Z",
          "iopub.execute_input": "2025-06-14T06:47:35.18542Z",
          "iopub.status.idle": "2025-06-14T06:47:38.124074Z",
          "shell.execute_reply.started": "2025-06-14T06:47:35.185395Z",
          "shell.execute_reply": "2025-06-14T06:47:38.123346Z"
        },
        "id": "Tg85vPjlTRec"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"/kaggle/input/mango-dataset-2012\"\n",
        "all_classes = os.listdir(dataset_path)\n",
        "\n",
        "print(f\"âœ… Total Mango Species: {len(all_classes)}\")\n",
        "print(\"ðŸ“¦ Classes:\", all_classes)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-14T06:47:38.125914Z",
          "iopub.execute_input": "2025-06-14T06:47:38.126224Z",
          "iopub.status.idle": "2025-06-14T06:47:38.133578Z",
          "shell.execute_reply.started": "2025-06-14T06:47:38.126206Z",
          "shell.execute_reply": "2025-06-14T06:47:38.132888Z"
        },
        "id": "ABVornTjTRec"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "for cls in all_classes:\n",
        "    files = glob(os.path.join(dataset_path, cls, \"*.jpg\"))\n",
        "    print(f\"{cls}: {len(files)} images\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-14T06:47:38.13435Z",
          "iopub.execute_input": "2025-06-14T06:47:38.134545Z",
          "iopub.status.idle": "2025-06-14T06:47:38.227965Z",
          "shell.execute_reply.started": "2025-06-14T06:47:38.134531Z",
          "shell.execute_reply": "2025-06-14T06:47:38.227288Z"
        },
        "id": "zwuPP8FRTRed"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class_counts = {cls: len(glob(os.path.join(dataset_path, cls, \"*.jpg\"))) for cls in all_classes}\n",
        "class_df = pd.DataFrame.from_dict(class_counts, orient='index', columns=['Image Count'])\n",
        "class_df.sort_values('Image Count', ascending=False).plot(kind='bar', figsize=(10, 5), color='orange', legend=False)\n",
        "plt.title(\"ðŸ“Š Mango Species Image Distribution\")\n",
        "plt.ylabel(\"Image Count\")\n",
        "plt.xlabel(\"Species\")\n",
        "plt.grid(axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-14T06:47:38.22879Z",
          "iopub.execute_input": "2025-06-14T06:47:38.229052Z",
          "iopub.status.idle": "2025-06-14T06:47:38.570663Z",
          "shell.execute_reply.started": "2025-06-14T06:47:38.229029Z",
          "shell.execute_reply": "2025-06-14T06:47:38.56986Z"
        },
        "id": "49LyEnUSTRed"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_random_images_per_class(dataset_path, classes, num_images=3):\n",
        "    fig, axes = plt.subplots(len(classes), num_images, figsize=(num_images*3, len(classes)*3))\n",
        "\n",
        "    for i, cls in enumerate(classes):\n",
        "        cls_path = os.path.join(dataset_path, cls)\n",
        "        images = random.sample(glob(os.path.join(cls_path, \"*.jpg\")), num_images)\n",
        "\n",
        "        for j in range(num_images):\n",
        "            img = Image.open(images[j])\n",
        "            axes[i, j].imshow(img)\n",
        "            axes[i, j].axis('off')\n",
        "            if j == 1:\n",
        "                axes[i, j].set_title(cls, fontsize=10)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_random_images_per_class(dataset_path, all_classes[:5])  # Limit to first 5 species for display\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-14T06:47:38.571439Z",
          "iopub.execute_input": "2025-06-14T06:47:38.571694Z",
          "iopub.status.idle": "2025-06-14T06:48:00.060112Z",
          "shell.execute_reply.started": "2025-06-14T06:47:38.571671Z",
          "shell.execute_reply": "2025-06-14T06:48:00.059408Z"
        },
        "id": "E3HSg7DtTRed"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "image_shapes = []\n",
        "\n",
        "for cls in all_classes:\n",
        "    cls_path = os.path.join(dataset_path, cls)\n",
        "    img_paths = glob(os.path.join(cls_path, \"*.jpg\"))\n",
        "\n",
        "    for img_path in random.sample(img_paths, min(20, len(img_paths))):  # Limit per class\n",
        "        img = Image.open(img_path)\n",
        "        image_shapes.append(img.size)\n",
        "\n",
        "shape_df = pd.DataFrame(image_shapes, columns=['Width', 'Height'])\n",
        "print(\"ðŸ§¾ Unique Resolutions:\\n\", shape_df.value_counts())\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.histplot(shape_df['Width'], color='skyblue', label='Width', kde=True)\n",
        "sns.histplot(shape_df['Height'], color='orange', label='Height', kde=True)\n",
        "plt.title(\"ðŸ“ Image Dimension Distribution\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-14T06:48:00.060994Z",
          "iopub.execute_input": "2025-06-14T06:48:00.061232Z",
          "iopub.status.idle": "2025-06-14T06:48:02.058054Z",
          "shell.execute_reply.started": "2025-06-14T06:48:00.061214Z",
          "shell.execute_reply": "2025-06-14T06:48:02.05721Z"
        },
        "id": "VKk6WfsMTRed"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def avg_rgb(image_path):\n",
        "    img = cv2.imread(image_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    return np.mean(img, axis=(0,1))  # RGB mean\n",
        "\n",
        "rgb_means = []\n",
        "labels = []\n",
        "\n",
        "for cls in all_classes[:5]:  # Limit for speed\n",
        "    paths = glob(os.path.join(dataset_path, cls, \"*.jpg\"))\n",
        "    for img_path in random.sample(paths, min(10, len(paths))):\n",
        "        rgb_means.append(avg_rgb(img_path))\n",
        "        labels.append(cls)\n",
        "\n",
        "rgb_df = pd.DataFrame(rgb_means, columns=['R', 'G', 'B'])\n",
        "rgb_df['Class'] = labels\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(data=rgb_df, x='R', y='G', hue='Class', palette='Set2', alpha=0.7)\n",
        "plt.title(\"ðŸŽ¨ RGB Space Distribution (Sampled)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-14T06:48:02.060179Z",
          "iopub.execute_input": "2025-06-14T06:48:02.060431Z",
          "iopub.status.idle": "2025-06-14T06:48:21.279584Z",
          "shell.execute_reply.started": "2025-06-14T06:48:02.060411Z",
          "shell.execute_reply": "2025-06-14T06:48:21.278788Z"
        },
        "id": "zsd1qnZnTRed"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "DATA_DIR = '/kaggle/input/mango-dataset-2012'\n",
        "BATCH_SIZE = 32\n",
        "NUM_WORKERS = 2  # Kaggle allows 2 workers safely\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-14T07:35:16.542695Z",
          "iopub.execute_input": "2025-06-14T07:35:16.542984Z",
          "iopub.status.idle": "2025-06-14T07:35:23.788771Z",
          "shell.execute_reply.started": "2025-06-14T07:35:16.542957Z",
          "shell.execute_reply": "2025-06-14T07:35:23.788203Z"
        },
        "id": "AnP2gTkqTRee"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Training transforms with augmentation\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Validation/test transforms without augmentation\n",
        "val_test_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-14T07:35:23.789972Z",
          "iopub.execute_input": "2025-06-14T07:35:23.79037Z",
          "iopub.status.idle": "2025-06-14T07:35:23.795358Z",
          "shell.execute_reply.started": "2025-06-14T07:35:23.790351Z",
          "shell.execute_reply": "2025-06-14T07:35:23.794549Z"
        },
        "id": "QUHmOMR3TRee"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "full_dataset = datasets.ImageFolder(root=DATA_DIR, transform=train_transforms)\n",
        "print(f\"Total images found: {len(full_dataset)}\")\n",
        "print(\"Classes found:\", full_dataset.classes)\n",
        "print(\"Class to index mapping:\", full_dataset.class_to_idx)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-14T07:35:23.796101Z",
          "iopub.execute_input": "2025-06-14T07:35:23.796387Z",
          "iopub.status.idle": "2025-06-14T07:35:27.53594Z",
          "shell.execute_reply.started": "2025-06-14T07:35:23.796361Z",
          "shell.execute_reply": "2025-06-14T07:35:27.535298Z"
        },
        "id": "xLuZtkixTRee"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "total_size = len(full_dataset)\n",
        "train_size = int(0.7 * total_size)\n",
        "val_size = int(0.15 * total_size)\n",
        "test_size = total_size - train_size - val_size\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# Override transforms for val and test subsets (no augmentation)\n",
        "val_dataset.dataset.transform = val_test_transforms\n",
        "test_dataset.dataset.transform = val_test_transforms\n",
        "\n",
        "print(f\"Train size: {len(train_dataset)}\")\n",
        "print(f\"Validation size: {len(val_dataset)}\")\n",
        "print(f\"Test size: {len(test_dataset)}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-14T07:35:27.536725Z",
          "iopub.execute_input": "2025-06-14T07:35:27.536975Z",
          "iopub.status.idle": "2025-06-14T07:35:27.543816Z",
          "shell.execute_reply.started": "2025-06-14T07:35:27.536959Z",
          "shell.execute_reply": "2025-06-14T07:35:27.542995Z"
        },
        "id": "RSluQhWQTRee"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-14T07:35:27.545431Z",
          "iopub.execute_input": "2025-06-14T07:35:27.545727Z",
          "iopub.status.idle": "2025-06-14T07:35:27.55682Z",
          "shell.execute_reply.started": "2025-06-14T07:35:27.545693Z",
          "shell.execute_reply": "2025-06-14T07:35:27.556095Z"
        },
        "id": "BWsESowWTRee"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def imshow(img_tensor):\n",
        "    img = img_tensor.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    img = std * img + mean\n",
        "    img = np.clip(img, 0, 1)\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = next(dataiter)\n",
        "print(f\"Batch size: {images.size(0)}\")\n",
        "print(f\"Labels: {labels}\")\n",
        "\n",
        "imshow(images[0])\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-14T07:35:27.557622Z",
          "iopub.execute_input": "2025-06-14T07:35:27.557858Z",
          "iopub.status.idle": "2025-06-14T07:35:33.721858Z",
          "shell.execute_reply.started": "2025-06-14T07:35:27.557839Z",
          "shell.execute_reply": "2025-06-14T07:35:33.72073Z"
        },
        "id": "6j7kkBtjTRee"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import timm\n",
        "from tqdm import tqdm\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Define hyperparameters\n",
        "num_classes = 10  # Based on 10 mango varieties\n",
        "num_epochs = 10   # Adjust as needed\n",
        "batch_size = 32   # Adjust based on memory constraints\n",
        "\n",
        "# Define transforms\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Models expect 224x224 input\n",
        "    transforms.RandomHorizontalFlip(),  # Augmentation for training\n",
        "    transforms.RandomRotation(10),      # Augmentation for training\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization\n",
        "])\n",
        "\n",
        "val_test_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load full dataset\n",
        "dataset_path = '/kaggle/input/mango-dataset-2012'\n",
        "full_dataset = ImageFolder(root=dataset_path, transform=train_transforms)\n",
        "\n",
        "# Split dataset\n",
        "total_size = len(full_dataset)\n",
        "train_size = int(0.7 * total_size)  # 1408 images\n",
        "val_size = int(0.15 * total_size)  # 301 images\n",
        "test_size = total_size - train_size - val_size  # 303 images\n",
        "train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# Override transforms for val and test subsets (no augmentation)\n",
        "val_dataset.dataset.transform = val_test_transforms\n",
        "test_dataset.dataset.transform = val_test_transforms\n",
        "\n",
        "# Verify dataset sizes and classes\n",
        "print(f\"Train size: {len(train_dataset)}\")\n",
        "print(f\"Validation size: {len(val_dataset)}\")\n",
        "print(f\"Test size: {len(test_dataset)}\")\n",
        "print(f\"Classes: {full_dataset.classes}\")\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "# Model definitions\n",
        "def get_mobilenet_v2(num_classes):\n",
        "    model = models.mobilenet_v2(weights='DEFAULT')\n",
        "    model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
        "    for param in model.features.parameters():\n",
        "        param.requires_grad = False\n",
        "    return model.to(device)\n",
        "\n",
        "def get_efficientnet_lite0(num_classes):\n",
        "    model = timm.create_model('efficientnet_lite0', pretrained=True)\n",
        "    model.classifier = nn.Linear(model.classifier.in_features, num_classes)\n",
        "    for name, param in model.named_parameters():\n",
        "        if \"classifier\" not in name:\n",
        "            param.requires_grad = False\n",
        "    return model.to(device)\n",
        "\n",
        "def get_squeezenet(num_classes):\n",
        "    model = models.squeezenet1_1(weights='DEFAULT')\n",
        "    model.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1))\n",
        "    model.num_classes = num_classes\n",
        "    for param in model.features.parameters():\n",
        "        param.requires_grad = False\n",
        "    return model.to(device)\n",
        "\n",
        "def get_shufflenet_v2(num_classes):\n",
        "    model = models.shufflenet_v2_x1_0(weights='DEFAULT')\n",
        "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "    for param in model.conv1.parameters():\n",
        "        param.requires_grad = False\n",
        "    for param in model.maxpool.parameters():\n",
        "        param.requires_grad = False\n",
        "    for param in model.stage2.parameters():\n",
        "        param.requires_grad = False\n",
        "    for param in model.stage3.parameters():\n",
        "        param.requires_grad = False\n",
        "    for param in model.stage4.parameters():\n",
        "        param.requires_grad = False\n",
        "    return model.to(device)\n",
        "\n",
        "# Dictionary of models to train\n",
        "models_dict = {\n",
        "    \"mobilenet_v2\": get_mobilenet_v2(num_classes),\n",
        "    \"efficientnet_lite0\": get_efficientnet_lite0(num_classes),\n",
        "    \"squeezenet\": get_squeezenet(num_classes),\n",
        "    \"shufflenet_v2\": get_shufflenet_v2(num_classes),\n",
        "}\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def train_one_epoch(model, optimizer, dataloader):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for inputs, labels in tqdm(dataloader, desc=\"Training\", leave=False):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # SqueezeNet outputs are [batch, classes, 1, 1]\n",
        "        if outputs.dim() == 4:\n",
        "            outputs = outputs.squeeze(-1).squeeze(-1)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / total if total > 0 else 0\n",
        "    epoch_acc = correct / total if total > 0 else 0\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "def validate(model, dataloader):\n",
        "    model.eval()\n",
        "    running_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(dataloader, desc=\"Validation\", leave=False):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            if outputs.dim() == 4:\n",
        "                outputs = outputs.squeeze(-1).squeeze(-1)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / total if total > 0 else 0\n",
        "    epoch_acc = correct / total if total > 0 else 0\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "def test_model(model, dataloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(dataloader, desc=\"Testing\", leave=False):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            if outputs.dim() == 4:\n",
        "                outputs = outputs.squeeze(-1).squeeze(-1)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    test_acc = correct / total if total > 0 else 0\n",
        "    return test_acc\n",
        "\n",
        "# Training loop for all models\n",
        "for model_name, model in models_dict.items():\n",
        "    print(f\"\\n--- Training {model_name} ---\")\n",
        "\n",
        "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\n",
        "\n",
        "    best_val_acc = 0\n",
        "    best_model_wts = None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        train_loss, train_acc = train_one_epoch(model, optimizer, train_loader)\n",
        "        val_loss, val_acc = validate(model, val_loader)\n",
        "\n",
        "        print(f\"Train loss: {train_loss:.4f}, Train acc: {train_acc:.4f}\")\n",
        "        print(f\"Val   loss: {val_loss:.4f}, Val   acc: {val_acc:.4f}\")\n",
        "\n",
        "        # Save best model weights based on validation accuracy\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_model_wts = model.state_dict()\n",
        "\n",
        "    # Save best model to disk\n",
        "    torch.save(best_model_wts, f\"{model_name}_best.pth\")\n",
        "    print(f\"Best val accuracy for {model_name}: {best_val_acc:.4f}\")\n",
        "    print(f\"Saved best model weights for {model_name}.\")\n",
        "\n",
        "    # Evaluate on test set\n",
        "    model.load_state_dict(best_model_wts)  # Load best weights\n",
        "    test_acc = test_model(model, test_loader)\n",
        "    print(f\"Test accuracy for {model_name}: {test_acc:.4f}\\n\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-14T07:46:19.900427Z",
          "iopub.execute_input": "2025-06-14T07:46:19.901308Z"
        },
        "id": "pKLTqjJ3TRee"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}